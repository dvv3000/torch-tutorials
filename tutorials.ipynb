{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\r\n",
    "import torchvision\r\n",
    "import torch.nn as nn\r\n",
    "import torchvision.transforms as transforms\r\n",
    "import numpy as np \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autograd "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "x = torch.rand(10, 3)\r\n",
    "x"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2964, 0.8620, 0.6770],\n",
       "        [0.3951, 0.7407, 0.5657],\n",
       "        [0.7447, 0.1209, 0.9605],\n",
       "        [0.6744, 0.5397, 0.7447],\n",
       "        [0.9279, 0.8498, 0.1893],\n",
       "        [0.8846, 0.9220, 0.9078],\n",
       "        [0.5299, 0.1957, 0.1977],\n",
       "        [0.2002, 0.8699, 0.2947],\n",
       "        [0.1395, 0.4538, 0.4965],\n",
       "        [0.7696, 0.4815, 0.8114]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "y = torch.rand(10, 2)\r\n",
    "y"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2606, 0.5114],\n",
       "        [0.7013, 0.8323],\n",
       "        [0.8353, 0.0243],\n",
       "        [0.0475, 0.7386],\n",
       "        [0.1597, 0.6014],\n",
       "        [0.9265, 0.9881],\n",
       "        [0.8976, 0.7063],\n",
       "        [0.3917, 0.1966],\n",
       "        [0.1826, 0.3504],\n",
       "        [0.7207, 0.2694]])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "linear = nn.Linear(3, 2)\r\n",
    "print('w: ', linear.weight)\r\n",
    "print('b: ', linear.bias)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[-0.3837, -0.3458,  0.2591],\n",
      "        [ 0.2627, -0.1784,  0.1059]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([ 0.4152, -0.4141], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "linear.parameters()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000002207A516F90>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "criterion = nn.MSELoss()\r\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "pred = linear(x)\r\n",
    "\r\n",
    "loss = criterion(pred, y)\r\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss after 1 step optimization:  0.5211935043334961\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading data from numpy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "x = np.array([[1, 2, 3], [5, 7, 9]])\r\n",
    "x"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [5, 7, 9]])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "y = torch.from_numpy(x)\r\n",
    "y"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [5, 7, 9]], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "z = y.numpy\r\n",
    "z"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function Tensor.numpy>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Input pipeline with data in torch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Download and construct mnist dataset.\r\n",
    "train_dataset = torchvision.datasets.MNIST(root='data/', train=True, transform=transforms.ToTensor(), download=True)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\dangv\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Fetch one data pair\r\n",
    "images, label = train_dataset[0]\r\n",
    "print(images.shape)\r\n",
    "print(label)\r\n",
    "plt.imshow(np.transpose(images, (1,2,0)))\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOX0lEQVR4nO3dbYxc5XnG8euKbUwxJvHGseMQFxzjFAg0Jl0ZkBFQoVCCIgGKCLGiiFBapwlOQutKUFoVWtHKrRIiSimSKS6m4iWQgPAHmsSyECRqcFmoAROHN+MS4+0aswIDIfZ6fffDjqsFdp5dZs68eO//T1rNzLnnzLk1cPmcmeeceRwRAjD5faDTDQBoD8IOJEHYgSQIO5AEYQeSmNrOjR3i6XGoZrRzk0Aqv9Fb2ht7PFatqbDbPkfS9ZKmSPrXiFhVev6hmqGTfVYzmwRQsDE21K01fBhve4qkGyV9TtLxkpbZPr7R1wPQWs18Zl8i6fmI2BoReyXdJem8atoCULVmwn6kpF+Nery9tuwdbC+33We7b0h7mtgcgGY0E/axvgR4z7m3EbE6InojoneapjexOQDNaCbs2yXNH/X445J2NNcOgFZpJuyPSlpke4HtQyR9SdK6atoCULWGh94iYp/tFZJ+rJGhtzUR8XRlnQGoVFPj7BHxgKQHKuoFQAtxuiyQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJNDWLK7qfp5b/E0/5yOyWbv+ZPz+6bm34sP3FdY9auLNYP+wbLtb/97pD6tYe7/1+cd1dw28V6yffs7JYP+bPHinWO6GpsNveJukNScOS9kVEbxVNAaheFXv234+IXRW8DoAW4jM7kESzYQ9JP7H9mO3lYz3B9nLbfbb7hrSnyc0BaFSzh/FLI2KH7TmS1tv+ZUQ8PPoJEbFa0mpJOsI90eT2ADSoqT17ROyo3e6UdJ+kJVU0BaB6DYfd9gzbMw/cl3S2pM1VNQagWs0cxs+VdJ/tA69zR0T8qJKuJpkpxy0q1mP6tGJ9xxkfKtbfPqX+mHDPB8vjxT/9dHm8uZP+49czi/V/+OdzivWNJ95Rt/bi0NvFdVcNfLZY/9hPD75PpA2HPSK2Svp0hb0AaCGG3oAkCDuQBGEHkiDsQBKEHUiCS1wrMHzmZ4r16269sVj/5LT6l2JOZkMxXKz/9Q1fLdanvlUe/jr1nhV1azNf3ldcd/qu8tDcYX0bi/VuxJ4dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnL0C05/ZUaw/9pv5xfonpw1U2U6lVvafUqxvfbP8U9S3LvxB3drr+8vj5HP/6T+L9VY6+C5gHR97diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IwhHtG1E8wj1xss9q2/a6xeAlpxbru88p/9zzlCcPL9af+MYN77unA67d9bvF+qNnlMfRh197vViPU+v/APG2bxVX1YJlT5SfgPfYGBu0OwbHnMuaPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4exeYMvvDxfrwq4PF+ot31B8rf/r0NcV1l/z9N4v1OTd27ppyvH9NjbPbXmN7p+3No5b12F5v+7na7awqGwZQvYkcxt8q6d2z3l8paUNELJK0ofYYQBcbN+wR8bCkdx9Hnidpbe3+WknnV9sWgKo1+gXd3Ijol6Ta7Zx6T7S93Haf7b4h7WlwcwCa1fJv4yNidUT0RkTvNE1v9eYA1NFo2Adsz5Ok2u3O6loC0AqNhn2dpItr9y+WdH817QBolXF/N972nZLOlDTb9nZJV0taJelu25dKeknSha1scrIb3vVqU+sP7W58fvdPffkXxforN00pv8D+8hzr6B7jhj0iltUpcXYMcBDhdFkgCcIOJEHYgSQIO5AEYQeSYMrmSeC4K56tW7vkxPKgyb8dtaFYP+PCy4r1md9/pFhH92DPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM4+CZSmTX7168cV131p3dvF+pXX3las/8UXLyjW478/WLc2/+9+XlxXbfyZ8wzYswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEkzZnNzgH55arN9+9XeK9QVTD21425+6bUWxvujm/mJ939ZtDW97smpqymYAkwNhB5Ig7EAShB1IgrADSRB2IAnCDiTBODuKYuniYv2IVduL9Ts/8eOGt33sg39UrP/O39S/jl+Shp/b2vC2D1ZNjbPbXmN7p+3No5ZdY/tl25tqf+dW2TCA6k3kMP5WSeeMsfx7EbG49vdAtW0BqNq4YY+IhyUNtqEXAC3UzBd0K2w/WTvMn1XvSbaX2+6z3TekPU1sDkAzGg37TZIWSlosqV/Sd+s9MSJWR0RvRPRO0/QGNwegWQ2FPSIGImI4IvZLulnSkmrbAlC1hsJue96ohxdI2lzvuQC6w7jj7LbvlHSmpNmSBiRdXXu8WFJI2ibpaxFRvvhYjLNPRlPmzinWd1x0TN3axiuuL677gXH2RV9+8exi/fXTXi3WJ6PSOPu4k0RExLIxFt/SdFcA2orTZYEkCDuQBGEHkiDsQBKEHUiCS1zRMXdvL0/ZfJgPKdZ/HXuL9c9/8/L6r33fxuK6Byt+ShoAYQeyIOxAEoQdSIKwA0kQdiAJwg4kMe5Vb8ht/2mLi/UXLixP2XzC4m11a+ONo4/nhsGTivXD7u9r6vUnG/bsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+yTnHtPKNaf/VZ5rPvmpWuL9dMPLV9T3ow9MVSsPzK4oPwC+8f9dfNU2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKMsx8Epi44qlh/4ZKP1a1dc9FdxXW/cPiuhnqqwlUDvcX6Q9efUqzPWlv+3Xm807h7dtvzbT9oe4vtp21/u7a8x/Z628/Vbme1vl0AjZrIYfw+SSsj4jhJp0i6zPbxkq6UtCEiFknaUHsMoEuNG/aI6I+Ix2v335C0RdKRks6TdOBcyrWSzm9RjwAq8L6+oLN9tKSTJG2UNDci+qWRfxAkzamzznLbfbb7hrSnyXYBNGrCYbd9uKQfSro8InZPdL2IWB0RvRHRO03TG+kRQAUmFHbb0zQS9Nsj4t7a4gHb82r1eZJ2tqZFAFUYd+jNtiXdImlLRFw3qrRO0sWSVtVu729Jh5PA1KN/u1h//ffmFesX/e2PivU/+dC9xXorrewvD4/9/F/qD6/13PpfxXVn7WdorUoTGWdfKukrkp6yvam27CqNhPxu25dKeknShS3pEEAlxg17RPxM0piTu0s6q9p2ALQKp8sCSRB2IAnCDiRB2IEkCDuQBJe4TtDUeR+tWxtcM6O47tcXPFSsL5s50FBPVVjx8mnF+uM3LS7WZ/9gc7He8wZj5d2CPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJJFmnH3vH5R/tnjvnw4W61cd80Dd2tm/9VZDPVVlYPjturXT160srnvsX/2yWO95rTxOvr9YRTdhzw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSaQZZ992fvnftWdPvKdl277xtYXF+vUPnV2se7jej/uOOPbaF+vWFg1sLK47XKxiMmHPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJOCLKT7DnS7pN0kc1cvny6oi43vY1kv5Y0iu1p14VEfUv+pZ0hHviZDPxK9AqG2ODdsfgmCdmTOSkmn2SVkbE47ZnSnrM9vpa7XsR8Z2qGgXQOhOZn71fUn/t/hu2t0g6stWNAajW+/rMbvtoSSdJOnAO5grbT9peY3tWnXWW2+6z3TekPc11C6BhEw677cMl/VDS5RGxW9JNkhZKWqyRPf93x1ovIlZHRG9E9E7T9OY7BtCQCYXd9jSNBP32iLhXkiJiICKGI2K/pJslLWldmwCaNW7YbVvSLZK2RMR1o5bPG/W0CySVp/ME0FET+TZ+qaSvSHrK9qbasqskLbO9WFJI2ibpay3oD0BFJvJt/M8kjTVuVxxTB9BdOIMOSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQxLg/JV3pxuxXJP3PqEWzJe1qWwPvT7f21q19SfTWqCp7OyoiPjJWoa1hf8/G7b6I6O1YAwXd2lu39iXRW6Pa1RuH8UAShB1IotNhX93h7Zd0a2/d2pdEb41qS28d/cwOoH06vWcH0CaEHUiiI2G3fY7tZ2w/b/vKTvRQj+1ttp+yvcl2X4d7WWN7p+3No5b12F5v+7na7Zhz7HWot2tsv1x77zbZPrdDvc23/aDtLbaftv3t2vKOvneFvtryvrX9M7vtKZKelfRZSdslPSppWUT8oq2N1GF7m6TeiOj4CRi2T5f0pqTbIuKE2rJ/lDQYEatq/1DOiogruqS3ayS92elpvGuzFc0bPc24pPMlfVUdfO8KfX1RbXjfOrFnXyLp+YjYGhF7Jd0l6bwO9NH1IuJhSYPvWnyepLW1+2s18j9L29XprStERH9EPF67/4akA9OMd/S9K/TVFp0I+5GSfjXq8XZ113zvIeknth+zvbzTzYxhbkT0SyP/80ia0+F+3m3cabzb6V3TjHfNe9fI9OfN6kTYx5pKqpvG/5ZGxGckfU7SZbXDVUzMhKbxbpcxphnvCo1Of96sToR9u6T5ox5/XNKODvQxpojYUbvdKek+dd9U1AMHZtCt3e7scD//r5um8R5rmnF1wXvXyenPOxH2RyUtsr3A9iGSviRpXQf6eA/bM2pfnMj2DElnq/umol4n6eLa/Ysl3d/BXt6hW6bxrjfNuDr83nV8+vOIaPufpHM18o38C5L+shM91OnrE5KeqP093eneJN2pkcO6IY0cEV0q6cOSNkh6rnbb00W9/bukpyQ9qZFgzetQb6dp5KPhk5I21f7O7fR7V+irLe8bp8sCSXAGHZAEYQeSIOxAEoQdSIKwA0kQdiAJwg4k8X+zhHFo7nUhhwAAAABJRU5ErkJggg=="
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "data_iter = iter(train_loader)\r\n",
    "# Mini-batch images and labels\r\n",
    "images, labels = next(data_iter)\r\n",
    "\r\n",
    "i = random.randint(0, 64)\r\n",
    "print(labels[i])\r\n",
    "plt.imshow(np.transpose(images[i], (1, 2, 0)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22809d68d30>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Actual usage of the data loader is as below.\r\n",
    "for images, labels in train_loader:\r\n",
    "    #Training code should be written here\r\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Input pipeline for custom dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Build a custom dataset below.\r\n",
    "\r\n",
    "class CustomDataset(torch.utils.data.Dataset):\r\n",
    "    def __init__(self):\r\n",
    "        #TODO\r\n",
    "        #1. Initialize file path or list of file name\r\n",
    "        pass\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        #TODO\r\n",
    "        #1. Read one data from file (eg: using numpy.fromfile, PIL.Image.open)\r\n",
    "        #2. Preprocess the data (eg: torchvision.transforms)\r\n",
    "        #3. Return a data pair (eg: image and label)\r\n",
    "        pass\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        # You should change 0 to the total size of your dataset\r\n",
    "        return 0\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Using the data loader\r\n",
    "custom_data = CustomDataset()\r\n",
    "custom_loader = torch.utils.data.DataLoader(dataset=custom_data, batch_size=64, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss function "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## L1 loss(x, y) = abs(x - y)\r\n",
    "### torch.nn.L1Loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "\r\n",
    "x = torch.tensor([[12, 82],[67, 7]], dtype=torch.float, requires_grad=True)\r\n",
    "y = torch.tensor([[65, 69],[48, 1]], dtype=torch.float)\r\n",
    "\r\n",
    "loss = nn.L1Loss()\r\n",
    "output = loss(x, y)\r\n",
    "output.backward()\r\n",
    "\r\n",
    "print(x)\r\n",
    "print(y)\r\n",
    "print(output)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[12., 82.],\n",
      "        [67.,  7.]], requires_grad=True)\n",
      "tensor([[65., 69.],\n",
      "        [48.,  1.]])\n",
      "tensor(22.7500, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MSE Loss  loss = (x - y)**2\r\n",
    "nn.MSELoss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "x = torch.tensor([[12, 82],[67, 7]], dtype=torch.float, requires_grad=True)\r\n",
    "y = torch.tensor([[65, 69],[48, 1]], dtype=torch.float)\r\n",
    "\r\n",
    "loss = nn.MSELoss(reduction='sum')\r\n",
    "output = loss(x, y)\r\n",
    "output.backward()\r\n",
    "\r\n",
    "print(x)\r\n",
    "print(y)\r\n",
    "print(output)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[12., 82.],\n",
      "        [67.,  7.]], requires_grad=True)\n",
      "tensor([[65., 69.],\n",
      "        [48.,  1.]])\n",
      "tensor(3375., grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Crossentropy loss = - sum(x*log(y))\r\n",
    "nn.CrossEntropyLoss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "x = torch.tensor([[12, 82], [67, 7]], dtype=torch.float, requires_grad=True)\r\n",
    "y = torch.tensor([0, 1], dtype=torch.long)\r\n",
    "\r\n",
    "loss = nn.CrossEntropyLoss()\r\n",
    "output = loss(x, y)\r\n",
    "output.backward()\r\n",
    "\r\n",
    "print(x)\r\n",
    "print(y)\r\n",
    "print(output)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[12., 82.],\n",
      "        [67.,  7.]], requires_grad=True)\n",
      "tensor([0, 1])\n",
      "tensor(65., grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Binary crossentropy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "\r\n",
    "loss = nn.BCELoss()\r\n",
    "x = torch.tensor([0.5, 0.6], dtype=torch.float, requires_grad=True)\r\n",
    "y = torch.tensor([0, 1], dtype=torch.float)\r\n",
    "\r\n",
    "sigmoid = nn.Sigmoid()\r\n",
    "output = loss(sigmoid(x), y)\r\n",
    "output.backward()\r\n",
    "\r\n",
    "print(sigmoid(x))\r\n",
    "print(y)\r\n",
    "print(output)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.6225, 0.6457], grad_fn=<SigmoidBackward>)\n",
      "tensor([0., 1.])\n",
      "tensor(0.7058, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BCEWithLogitsLoss "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "loss = nn.BCEWithLogitsLoss()\r\n",
    "x = torch.tensor([0.5, 0.6], dtype=torch.float, requires_grad=True)\r\n",
    "y = torch.tensor([0, 1], dtype=torch.float)\r\n",
    "output = loss(x, y)\r\n",
    "output.backward()\r\n",
    "\r\n",
    "print(x)\r\n",
    "print(y)\r\n",
    "\r\n",
    "print(output)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.5000, 0.6000], requires_grad=True)\n",
      "tensor([0., 1.])\n",
      "tensor(0.7058, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}